from keras.layers.core import Dense, Dropout, Activation
from keras.layers.convolutional import Conv2D, MaxPooling2D
from keras.layers import Input, BatchNormalization, Add, GlobalAveragePooling2D
from keras.models import Model

## shallower & wider ResNet Model A: SW-ResNet-A (https://arxiv.org/pdf/1611.10080.pdf)
def sw_resnet_a(IMG_H, IMG_W, IMG_C):
    # input
    input_tensor = Input(shape = (IMG_H, IMG_W, IMG_C))
    # 1st conv
    h_conv1 = BatchNormalization()(input_tensor)
    h_conv1 = Activation('relu')(h_conv1)
    h_conv1 = Conv2D(64, (3, 3), padding = 'same')(h_conv1)
    # 1st pooling
    h_pool1 = MaxPooling2D((3, 3), (2, 2), padding = 'same')(h_conv1)
    h_pool1 = BatchNormalization()(h_pool1)
    h_pool1 = Activation('relu')(h_pool1)
    h_pool1 = Conv2D(128, (1, 1), padding='same')(h_pool1)
    # B2_1
    h2_1 = BatchNormalization()(h_pool1)
    h2_1 = Activation('relu')(h2_1)
    h2_1 = Conv2D(128, (3, 3), padding='same')(h2_1)
    h2_1 = BatchNormalization()(h2_1)
    h2_1 = Activation('relu')(h2_1)
    h2_1 = Conv2D(128, (3, 3), padding='same')(h2_1)
    h_B2_1 = Add()([h_pool1, h2_1])
    # B2_2
    h2_2 = BatchNormalization()(h_B2_1)
    h2_2 = Activation('relu')(h2_2)
    h2_2 = Conv2D(128, (3, 3), padding='same')(h2_2)
    h2_2 = BatchNormalization()(h2_2)
    h2_2 = Activation('relu')(h2_2)
    h2_2 = Conv2D(128, (3, 3), padding='same')(h2_2)
    h_B2_2 = Add()([h_B2_1, h2_2])
    # B2_3
    h2_3 = BatchNormalization()(h_B2_2)
    h2_3 = Activation('relu')(h2_3)
    h2_3 = Conv2D(128, (3, 3), padding='same')(h2_3)
    h2_3 = BatchNormalization()(h2_3)
    h2_3 = Activation('relu')(h2_3)
    h2_3 = Conv2D(128, (3, 3), padding='same')(h2_3)
    h_B2_3 = Add()([h_B2_2, h2_3])
    # 2nd pooling
    h_pool2 = MaxPooling2D((3, 3), (2, 2), padding = 'same')(h_B2_3)
    h_pool2 = BatchNormalization()(h_pool2)
    h_pool2 = Activation('relu')(h_pool2)
    h_pool2 = Conv2D(256, (1, 1), padding='same')(h_pool2)
    # B3_1
    h3_1 = BatchNormalization()(h_pool2)
    h3_1 = Activation('relu')(h3_1)
    h3_1 = Conv2D(256, (3, 3), padding='same')(h3_1)
    h3_1 = BatchNormalization()(h3_1)
    h3_1 = Activation('relu')(h3_1)
    h3_1 = Conv2D(256, (3, 3), padding='same')(h3_1)
    h_B3_1 = Add()([h_pool2, h3_1])
    # B3_2
    h3_2 = BatchNormalization()(h_B3_1)
    h3_2 = Activation('relu')(h3_2)
    h3_2 = Conv2D(256, (3, 3), padding='same')(h3_2)
    h3_2 = BatchNormalization()(h3_2)
    h3_2 = Activation('relu')(h3_2)
    h3_2 = Conv2D(256, (3, 3), padding='same')(h3_2)
    h_B3_2 = Add()([h_B3_1, h3_2])
    # B3_3
    h3_3 = BatchNormalization()(h_B3_2)
    h3_3 = Activation('relu')(h3_3)
    h3_3 = Conv2D(256, (3, 3), padding='same')(h3_3)
    h3_3 = BatchNormalization()(h3_3)
    h3_3 = Activation('relu')(h3_3)
    h3_3 = Conv2D(256, (3, 3), padding='same')(h3_3)
    h_B3_3 = Add()([h_B3_2, h3_3])
    # 3rd pooling
    h_pool3 = MaxPooling2D((3, 3), (2, 2), padding = 'same')(h_B3_3)
    h_pool3 = BatchNormalization()(h_pool3)
    h_pool3 = Activation('relu')(h_pool3)
    h_pool3 = Conv2D(512, (1, 1), padding='same')(h_pool3)
    # B4_1
    h4_1 = BatchNormalization()(h_pool3)
    h4_1 = Activation('relu')(h4_1)
    h4_1 = Conv2D(512, (3, 3), padding='same')(h4_1)
    h4_1 = BatchNormalization()(h4_1)
    h4_1 = Activation('relu')(h4_1)
    h4_1 = Conv2D(512, (3, 3), padding='same')(h4_1)
    h_B4_1 = Add()([h_pool3, h4_1])
    # B4_2
    h4_2 = BatchNormalization()(h_B4_1)
    h4_2 = Activation('relu')(h4_2)
    h4_2 = Conv2D(512, (3, 3), padding='same')(h4_2)
    h4_2 = BatchNormalization()(h4_2)
    h4_2 = Activation('relu')(h4_2)
    h4_2 = Conv2D(512, (3, 3), padding='same')(h4_2)
    h_B4_2 = Add()([h_B4_1, h4_2])
    # B4_3
    h4_3 = BatchNormalization()(h_B4_2)
    h4_3 = Activation('relu')(h4_3)
    h4_3 = Conv2D(512, (3, 3), padding='same')(h4_3)
    h4_3 = BatchNormalization()(h4_3)
    h4_3 = Activation('relu')(h4_3)
    h4_3 = Conv2D(512, (3, 3), padding='same')(h4_3)
    h_B4_3 = Add()([h_B4_2, h4_3])
    # B4_4
    h4_4 = BatchNormalization()(h_B4_3)
    h4_4 = Activation('relu')(h4_4)
    h4_4 = Conv2D(512, (3, 3), padding='same')(h4_4)
    h4_4 = BatchNormalization()(h4_4)
    h4_4 = Activation('relu')(h4_4)
    h4_4 = Conv2D(512, (3, 3), padding='same')(h4_4)
    h_B4_4 = Add()([h_B4_3, h4_4])
    # B4_5
    h4_5 = BatchNormalization()(h_B4_4)
    h4_5 = Activation('relu')(h4_5)
    h4_5 = Conv2D(512, (3, 3), padding='same')(h4_5)
    h4_5 = BatchNormalization()(h4_5)
    h4_5 = Activation('relu')(h4_5)
    h4_5 = Conv2D(512, (3, 3), padding='same')(h4_5)
    h_B4_5 = Add()([h_B4_4, h4_5])
    # B4_6
    h4_6 = BatchNormalization()(h_B4_5)
    h4_6 = Activation('relu')(h4_6)
    h4_6 = Conv2D(512, (3, 3), padding='same')(h4_6)
    h4_6 = BatchNormalization()(h4_6)
    h4_6 = Activation('relu')(h4_6)
    h4_6 = Conv2D(512, (3, 3), padding='same')(h4_6)
    h_B4_6 = Add()([h_B4_5, h4_6])
    # 4th pooling
    h_pool4 = MaxPooling2D((3, 3), (2, 2), padding = 'same')(h_B4_6)
    h_pool4 = BatchNormalization()(h_pool4)
    h_pool4 = Activation('relu')(h_pool4)
    h_pool4 = Conv2D(1024, (1, 1), padding='same')(h_pool4)
    # B5_1
    h5_1 = BatchNormalization()(h_pool4)
    h5_1 = Activation('relu')(h5_1)
    h5_1 = Conv2D(512, (3, 3), padding='same')(h5_1)
    h5_1 = BatchNormalization()(h5_1)
    h5_1 = Activation('relu')(h5_1)
    h5_1 = Conv2D(1024, (3, 3), padding='same')(h5_1)
    h_B5_1 = Add()([h_pool4, h5_1])
    # B5_2
    h5_2 = BatchNormalization()(h_pool4)
    h5_2 = Activation('relu')(h5_2)
    h5_2 = Conv2D(512, (3, 3), padding='same')(h5_2)
    h5_2 = BatchNormalization()(h5_2)
    h5_2 = Activation('relu')(h5_2)
    h5_2 = Conv2D(1024, (3, 3), padding='same')(h5_2)
    h_B5_2 = Add()([h_B5_1, h5_2])
    # B5_3
    h5_3 = BatchNormalization()(h_pool4)
    h5_3 = Activation('relu')(h5_3)
    h5_3 = Conv2D(512, (3, 3), padding='same')(h5_3)
    h5_3 = BatchNormalization()(h5_3)
    h5_3 = Activation('relu')(h5_3)
    h5_3 = Conv2D(1024, (3, 3), padding='same')(h5_3)
    h_B5_3 = Add()([h_B5_2, h5_3])
    # 5th pooling
    h_pool5 = MaxPooling2D((3, 3), (2, 2), padding = 'same')(h_B5_3)
    h_pool5 = BatchNormalization()(h_pool5)
    h_pool5 = Activation('relu')(h_pool5)
    h_pool5 = Conv2D(2048, (1, 1), padding='same')(h_pool5)
    # B6
    h6 = BatchNormalization()(h_pool5)
    h6 = Activation('relu')(h6)
    h6 = Conv2D(512, (1, 1), padding='same')(h6)
    h6 = BatchNormalization()(h6)
    h6 = Activation('relu')(h6)
    h6 = Conv2D(1024, (3, 3), padding='same')(h6)
    h6 = BatchNormalization()(h6)
    h6 = Activation('relu')(h6)
    h6 = Conv2D(2048, (1, 1), padding='same')(h6)
    h_B6 = Add()([h_pool5, h6])
    h_B6 = BatchNormalization()(h_B6)
    h_B6 = Activation('relu')(h_B6)
    h_B6 = Conv2D(4096, (1, 1), padding='same')(h_B6)
    # B7
    h7 = BatchNormalization()(h_B6)
    h7 = Activation('relu')(h7)
    h7 = Conv2D(1024, (1, 1), padding='same')(h7)
    h7 = BatchNormalization()(h7)
    h7 = Activation('relu')(h7)
    h7 = Conv2D(2048, (3, 3), padding='same')(h7)
    h7 = BatchNormalization()(h7)
    h7 = Activation('relu')(h7)
    h7 = Conv2D(4096, (1, 1), padding='same')(h7)
    h_B7 = Add()([h_B6, h7])
    # classifier
    f = GlobalAveragePooling2D()(h_B7)
    output_tensor = Dense(1000, activation = 'softmax')(f)

    return Model(input_tensor, output_tensor)